{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and saving the cleaned twitter data\n",
    "* We load the cleaned dataframes saved in the twitter_cleaning notebook and do some further analysis. In particular, we tokenize the tweets and remove the stopwords so that we can do some analysis on word frequencies.\n",
    "* The dataframe with tokenized teets is saved as 'twitter_data/twitter_forFreq.parquet' and can be loaded directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import pyspark.sql\n",
    "from pyspark.sql.functions import col, udf\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "conf = SparkConf().setAppName(\"ADA-gcl\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the paqueret data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'file:///home/kirtan/Academics/EPFL/sem1/ADA/ADA-Project/twitter_data/final_twitter1.parquet'\n",
    "df = sqlContext.read.parquet(path)\n",
    "df = df.withColumn('canton', df.canton_)\n",
    "df = df.withColumn('sentiment', df.sentiment_)\n",
    "df = df.drop(df.canton_).drop(df.sentiment_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------------------+-------+-----------+-----+---+----------+---------+\n",
      "|                 id|language|                main| gender|time_period|month|tmp|    canton|sentiment|\n",
      "+-------------------+--------+--------------------+-------+-----------+-----+---+----------+---------+\n",
      "|1451606943000012161|      en|@NaliniSingh we a...|UNKNOWN|      Night|    1|  1|      Vaud|        0|\n",
      "|1451607568000005493|      en|If bae gets any s...| FEMALE|      Night|    1|  1|      Vaud|        0|\n",
      "|1451609302000001820|      en|Listening to \"Bre...|   MALE|      Night|    1|  1|    Zurich|        0|\n",
      "|1451618447000009640|      en|@Strippin @dexbon...|UNKNOWN|      Night|    1|  1|      Vaud|        0|\n",
      "|1451619941000014002|      en|@VictoriaJustice ...| FEMALE|      Night|    1|  1|      Vaud|        0|\n",
      "|1451623493000004879|      en|10NL Folded flush...| FEMALE|      Night|    1|  1|      Vaud|        0|\n",
      "|1451632164000001178|      en|@Iam_boika I love...| FEMALE|        Day|    1|  1|    Geneva|        1|\n",
      "|1451640244000006984|      en|Kanye West releas...|UNKNOWN|        Day|    1|  1|      Vaud|        0|\n",
      "|1451643361000012166|      en|@LiviaQuinn #than...|   MALE|        Day|    1|  1|      Vaud|        0|\n",
      "|1451644044000004409|      en|#Fraud Risk Manag...|UNKNOWN|        Day|    1|  1|Basel-City|        0|\n",
      "|1451644572000013487|      en|Charlie Puth â€“ On...|UNKNOWN|        Day|    1|  1|      Vaud|        0|\n",
      "|1451647098000005420|      en|Love Beauty Succe...|UNKNOWN|        Day|    1|  1|      Vaud|        0|\n",
      "|1451655503000015577|      en|Want to #quit #sm...|UNKNOWN|        Day|    1|  1|      Vaud|        0|\n",
      "|1451657261000013512|      en|\"Your are what yo...|UNKNOWN|        Day|    1|  1|      Vaud|        0|\n",
      "|1451657744000008985|      en|Thus, \"Moderate I...|   MALE|        Day|    1|  1|      Vaud|        0|\n",
      "|1451659256000015015|      en|#nowplaying Haelo...|UNKNOWN|        Day|    1|  1|      Vaud|        0|\n",
      "|1451660028000015056|      en|\"@__arsenalnews_:...|UNKNOWN|        Day|    1|  1|      Bern|        0|\n",
      "|1451660618000007737|      en|@HeadLineNoise ne...|   MALE|        Day|    1|  1|    Zurich|        0|\n",
      "|1451661029000012240|      en|Happy 2016 to all...| FEMALE|        Day|    1|  1|      Vaud|        0|\n",
      "|1451661266000004367|      en|Beautiful Swiss b...|UNKNOWN|        Day|    1|  1|    Zurich|        0|\n",
      "+-------------------+--------+--------------------+-------+-----------+-----+---+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get hashtags\n",
    "def get_hashtags(twt):\n",
    "    return(re.findall(r\"#(\\w+)\", twt))\n",
    "\n",
    "udfTags = udf(get_hashtags, ArrayType(StringType()))\n",
    "df = df.withColumn('tags', udfTags(\"main\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get stopwords and clean main\n",
    "stpWords = set(stopwords.words(\"english\")).union(set(stopwords.words(\"french\")).union(set(stopwords.words(\"german\"))))\n",
    "def clean_main(twt):\n",
    "    tkns = re.sub(\"(@[A-Za-z0-9_]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",' '.join(twt)).split()\n",
    "    tkns_l = [w.lower() for w in tkns]\n",
    "    return([word for word in tkns_l if word not in stpWords])  \n",
    "\n",
    "udfTokenize = udf(clean_main, ArrayType(StringType()))\n",
    "df = df.withColumn('tweet', udfTokenize(\"main\"))\n",
    "df = df.drop(df.main).drop(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def concat(type):\n",
    "    def concat_(*args):\n",
    "        return list(chain(*args))\n",
    "    return udf(concat_, ArrayType(type))\n",
    "\n",
    "concat_string_arrays = concat(StringType())\n",
    "\n",
    "df = df.withColumn('keywords', concat_string_arrays(col(\"tweet\"), col(\"tags\")))\n",
    "df = df.drop(df.tags).drop(df.keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+-----+---+----------+---------+--------------------+\n",
      "|language| gender|time_period|month|tmp|    canton|sentiment|               tweet|\n",
      "+--------+-------+-----------+-----+---+----------+---------+--------------------+\n",
      "|      en|UNKNOWN|      Night|    1|  1|      Vaud|        0|[anau, fishing, t...|\n",
      "|      en| FEMALE|      Night|    1|  1|      Vaud|        0|[bae, gets, smoot...|\n",
      "|      en|   MALE|      Night|    1|  1|    Zurich|        0|[listening, bread...|\n",
      "|      en|UNKNOWN|      Night|    1|  1|      Vaud|        0|[jesus, didnt, re...|\n",
      "|      en| FEMALE|      Night|    1|  1|      Vaud|        0|[vic, take, look,...|\n",
      "|      en| FEMALE|      Night|    1|  1|      Vaud|        0|[nl, folded, flus...|\n",
      "|      en| FEMALE|        Day|    1|  1|    Geneva|        1|        [love, papi]|\n",
      "|      en|UNKNOWN|        Day|    1|  1|      Vaud|        0|[kanye, west, rel...|\n",
      "|      en|   MALE|        Day|    1|  1|      Vaud|        0|[thanks, retweet,...|\n",
      "|      en|UNKNOWN|        Day|    1|  1|Basel-City|        0|[fraud, risk, man...|\n",
      "|      en|UNKNOWN|        Day|    1|  1|      Vaud|        0|[charlie, puth, o...|\n",
      "|      en|UNKNOWN|        Day|    1|  1|      Vaud|        0|[love, beauty, su...|\n",
      "|      en|UNKNOWN|        Day|    1|  1|      Vaud|        0|[want, quit, smok...|\n",
      "|      en|UNKNOWN|        Day|    1|  1|      Vaud|        0|[believe, paulo, ...|\n",
      "|      en|   MALE|        Day|    1|  1|      Vaud|        0|[thus, moderate, ...|\n",
      "|      en|UNKNOWN|        Day|    1|  1|      Vaud|        0|[nowplaying, hael...|\n",
      "|      en|UNKNOWN|        Day|    1|  1|      Bern|        0|[seeing, boys, li...|\n",
      "|      en|   MALE|        Day|    1|  1|    Zurich|        0|      [neither, wtc]|\n",
      "|      en| FEMALE|        Day|    1|  1|      Vaud|        0|[happy, friends, ...|\n",
      "|      en|UNKNOWN|        Day|    1|  1|    Zurich|        0|[beautiful, swiss...|\n",
      "+--------+-------+-----------+-----+---+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe botained above is saved as 'twitter_data/twitter_forFreq.parquet' so it can be loaded directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just read dataframe directly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.parquet('file:///home/kirtan/Academics/EPFL/sem1/ADA/ADA-Project/twitter_data/twitter_forFreq.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter the dataframe by given word\n",
    "def filter_df(df, keywords, column):\n",
    "    def check_L1_in_L2(l2):\n",
    "        for l in keywords:\n",
    "            if l in l2:\n",
    "                return True\n",
    "        return False\n",
    "    filt = udf(check_L1_in_L2, BooleanType())\n",
    "    return(df.where(filt(col(column))))\n",
    "\n",
    "# Aggregate dataframe by 'column' and calculate weighted mean of sentiment\n",
    "def get_grouped(df, column):\n",
    "    return(df.groupBy(column).agg((func.sum(df.sentiment)/func.sum(df.tmp)).alias('avg_sentiment'), func.sum(df.tmp).alias('count')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------+\n",
      "|              canton|      avg_sentiment|  count|\n",
      "+--------------------+-------------------+-------+\n",
      "|                Bern| 0.2026460148434979|  68178|\n",
      "|             Lucerne| 0.2885554149340083|  11062|\n",
      "|           NeuchÃ¢tel|0.26464646464646463|   3465|\n",
      "|Appenzell Innerrh...| 0.1276595744680851|     47|\n",
      "|        Schaffhausen| 0.3213728549141966|   1923|\n",
      "|                Vaud|0.19293273025541155|1762293|\n",
      "|              Ticino| 0.1091281351305238|  29305|\n",
      "|                 Uri|0.23214285714285715|     56|\n",
      "|              Valais|0.22400329871401695| 116409|\n",
      "|                Jura|0.23194748358862144|    457|\n",
      "|            Obwalden|0.13312693498452013|   1292|\n",
      "|              Geneva| 0.1413494047751649| 583561|\n",
      "|             Thurgau|0.19573400250941028|    797|\n",
      "|           Nidwalden|            0.33125|    160|\n",
      "|           Solothurn|0.28759493670886077|   3950|\n",
      "|          Basel-City| 0.2411903027091475| 241552|\n",
      "|              Zurich| 0.2431137485795549| 601924|\n",
      "|              Schwyz|  0.473972602739726|    365|\n",
      "|              Glarus| 0.5483870967741935|     31|\n",
      "|Appenzell Ausserr...| 0.5365239294710328|    397|\n",
      "+--------------------+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_grouped(df, 'canton').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filters the df by keywords from column tweet and gives average sentiment grouped by column\n",
    "def get_final_df(df, keywords, column, grp_column):\n",
    "    return(get_grouped(filter_df(df, keywords, column), grp_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+\n",
      "|    canton|       avg_sentiment|count|\n",
      "+----------+--------------------+-----+\n",
      "|      Bern|                 0.0|    4|\n",
      "|   Lucerne|                 0.0|    1|\n",
      "|      Vaud| 0.31266245822502786| 2693|\n",
      "|    Ticino|  0.6666666666666666|    6|\n",
      "|    Valais| 0.16666666666666666|   18|\n",
      "|    Geneva|0.038461538461538464|   26|\n",
      "| Solothurn|                -0.5|    2|\n",
      "|Basel-City| 0.27835051546391754|   97|\n",
      "|    Zurich| 0.15897435897435896|  195|\n",
      "|  Fribourg|                 0.0|    5|\n",
      "|    Aargau|                -1.0|    1|\n",
      "|   Grisons|                 0.0|    2|\n",
      "+----------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This looks for all the tweets which have the word 'federer' and groups by canton to \n",
    "# give the average sentiment for each canton.\n",
    "get_final_df(df, ['federer'], 'tweet','canton').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+\n",
      "|month|       avg_sentiment|count|\n",
      "+-----+--------------------+-----+\n",
      "|    1|0.011363636363636364|  264|\n",
      "|    2| 0.05573248407643312|  628|\n",
      "|    3| 0.04885496183206107|  655|\n",
      "|    4| 0.04983388704318937|  602|\n",
      "|    5| 0.06732673267326733|  505|\n",
      "|    6|0.053061224489795916|  490|\n",
      "|    7| 0.08074534161490683|  644|\n",
      "|    8| 0.13218884120171673| 1165|\n",
      "|    9| 0.13898026315789475| 1216|\n",
      "|   10| 0.16253968253968254| 1575|\n",
      "+-----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This looks for all the tweets which have either of the words in the array and then gives average sentiment by month.\n",
    "get_final_df(df, ['india', 'inde'], 'tweet', 'month').show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
